# Nano-vLLM Learning Path: Master LLM Inference Optimization

**Author**: Claude Code Analysis
**Target Audience**: Principal SDE / Senior ML Engineers focusing on inference optimization
**Repository**: nano-vLLM (~1,200 lines, production-level performance)
**Last Updated**: 2026-02-05

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Repository Overview](#repository-overview)
3. [Learning Path Report](#learning-path-report)
4. [Detailed Learning Path](#detailed-learning-path)
5. [Success Metrics](#success-metrics)
6. [Additional Resources](#additional-resources)

---

## Executive Summary

**Total Time Investment**: 8-10 days (60-80 hours of focused study)
**Difficulty Level**: Advanced (requires strong Python, PyTorch, and systems programming background)
**Primary Focus**: Production-grade LLM inference optimization techniques
**End Goal**: Deep understanding of modern serving systems (vLLM, SGLang, TensorRT-LLM)

### Why This Path is Effective

This learning path is **uniquely efficient** for mastering LLM inference because:

1. **Minimal Complexity**: 1,200 lines vs vLLM's 50K+ lines - you can hold the entire system in your head
2. **Real Performance**: Achieves parity with production vLLM, so techniques aren't toy examples
3. **Modern Stack**: Uses 2024-2025 best practices (Flash Attention v2, torch.compile, CUDA graphs)
4. **Hands-on Focus**: Every phase includes experiments, not just reading

### Quick Start

```bash
# Verify your environment
python example.py  # Should run successfully

# Start Phase 1
# Read this document, then begin with llm.py and engine/llm_engine.py
```

---

## Repository Overview

### Architecture & Components

Nano-vLLM is a lightweight, from-scratch implementation that achieves comparable or better inference speeds than full vLLM. The codebase is organized into four main modules:

```
nanovllm/
‚îú‚îÄ‚îÄ engine/          # Core inference engine
‚îÇ   ‚îú‚îÄ‚îÄ llm_engine.py      # Main orchestrator (94 lines)
‚îÇ   ‚îú‚îÄ‚îÄ scheduler.py       # Request scheduling (72 lines)
‚îÇ   ‚îú‚îÄ‚îÄ model_runner.py    # Model execution (253 lines)
‚îÇ   ‚îú‚îÄ‚îÄ block_manager.py   # KV cache management (113 lines)
‚îÇ   ‚îî‚îÄ‚îÄ sequence.py        # Request representation
‚îú‚îÄ‚îÄ layers/          # Model components and optimizations
‚îÇ   ‚îú‚îÄ‚îÄ attention.py       # FlashAttention integration (76 lines)
‚îÇ   ‚îú‚îÄ‚îÄ linear.py          # Tensor parallelism layers (154 lines)
‚îÇ   ‚îú‚îÄ‚îÄ rotary_embedding.py
‚îÇ   ‚îú‚îÄ‚îÄ sampler.py
‚îÇ   ‚îú‚îÄ‚îÄ layernorm.py
‚îÇ   ‚îî‚îÄ‚îÄ activation.py
‚îú‚îÄ‚îÄ models/          # Model implementations
‚îÇ   ‚îî‚îÄ‚îÄ qwen3.py           # Qwen3 model (217 lines)
‚îú‚îÄ‚îÄ utils/           # Helper utilities
‚îÇ   ‚îî‚îÄ‚îÄ loader.py          # Weight loading
‚îú‚îÄ‚îÄ config.py        # Configuration management
‚îú‚îÄ‚îÄ llm.py           # High-level API
‚îî‚îÄ‚îÄ sampling_params.py # Sampling parameters
```

### High-Level Execution Flow

1. User calls `LLM.generate()` with prompts and sampling parameters
2. **LLMEngine** orchestrates the inference process
3. **Scheduler** manages request batching and KV cache allocation
4. **ModelRunner** executes the actual model forward passes
5. **BlockManager** manages KV cache memory using prefix caching

### Key Optimization Techniques Implemented

| Technique | Location | Impact |
|-----------|----------|--------|
| **Prefix Caching** | BlockManager | Reuses KV cache for repeated prompts |
| **CUDA Graphs** | ModelRunner | Eliminates CPU overhead in decode phase |
| **Flash Attention** | Attention | 3-5x speedup on attention computation |
| **Torch Compile** | Sampler, RMSNorm, etc. | JIT optimization of kernels |
| **Tensor Parallelism** | Linear layers, Embedding | Multi-GPU scaling |
| **Fused Operations** | RMSNorm, SiluAndMul, LayerNorm | Reduced memory bandwidth |
| **Variable-length Attention** | ModelRunner.prepare_prefill | Efficient batching of different sequence lengths |
| **KV Cache Block Allocation** | BlockManager | Memory-efficient cache management |

### Performance Characteristics

**Benchmark Results** (RTX 4070 Laptop, Qwen3-0.6B):
- **vLLM**: 1361.84 tok/s
- **Nano-vLLM**: 1434.13 tok/s (+5.3% faster)
- Total: 256 sequences, ~134K output tokens

Key factors:
1. Lower overhead from simpler codebase
2. Efficient CUDA graph compilation
3. Optimized scheduling for batching

---

## Learning Path Report

### Value Proposition

After completing this path, you will be able to:

- ‚úÖ Explain why nano-vLLM is faster than naive implementations
- ‚úÖ Calculate KV cache memory requirements for any model/sequence length
- ‚úÖ Profile and identify bottlenecks in inference pipelines
- ‚úÖ Implement basic tensor parallelism for new layers
- ‚úÖ Design scheduling strategies for different workload patterns
- ‚úÖ Understand tradeoffs between throughput and latency

### Phase-by-Phase Breakdown

#### Phase 1: Architecture & System Design (8-10 hours)
**What You'll Learn:**
- Request lifecycle from API to GPU execution
- How LLM serving differs from training
- The separation of concerns: scheduler, executor, memory manager

**Concrete Deliverables:**
- Flow diagram of request processing
- Understanding of `generate()` ‚Üí tokens pipeline
- Ability to trace a single request through the codebase

**Key Files**: 4 files, ~200 lines total
**Difficulty**: ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ (Straightforward architecture study)

---

#### Phase 2: The Scheduling Problem (10-12 hours)
**What You'll Learn:**
- Why prefill and decode phases must be separated
- Block-based KV cache management (foundation of PagedAttention)
- Prefix caching for cost-free prompt sharing
- Preemption strategies when memory is exhausted

**Concrete Deliverables:**
- Modified scheduler with custom prioritization logic
- Cache hit rate monitoring dashboard
- Analysis of block allocation patterns for different workloads

**Key Insights:**
- Memory is the bottleneck, not compute (in decode phase)
- Scheduling is a multi-constraint optimization problem
- Reference counting enables zero-copy sharing

**Key Files**: 3 files, ~280 lines
**Difficulty**: ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ (Most conceptually challenging phase)

---

#### Phase 3: GPU Execution Optimization (12-16 hours)
**What You'll Learn:**
- CUDA graphs: How to eliminate CPU overhead
- Flash Attention: Why it's 3-5x faster than naive attention
- Variable-length batching strategies
- When and why to use torch.compile

**Concrete Deliverables:**
- Profiling report comparing eager vs CUDA graph execution
- Performance breakdown of Flash Attention impact
- Custom torch.compile kernels for new operations

**Benchmark Results You'll Generate:**
```
Configuration          | Tokens/sec | GPU Util | Latency (p50)
-----------------------|------------|----------|---------------
Baseline (no opts)     |    450     |   45%    |   89ms
+ Flash Attention      |   1100     |   78%    |   42ms
+ CUDA Graphs          |   1420     |   82%    |   31ms
```

**Key Files**: 2 files, ~330 lines
**Difficulty**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Performance-critical, GPU programming concepts)

---

#### Phase 4: Tensor Parallelism (10-12 hours)
**What You'll Learn:**
- How to shard model weights across GPUs
- NCCL communication patterns
- All-reduce vs all-gather strategies
- Process synchronization with multiprocessing

**Concrete Deliverables:**
- Scaling analysis: 1 vs 2 vs 4 vs 8 GPUs
- Communication overhead measurements
- Weight sharding implementation for new model

**Scaling Efficiency You'll Measure:**
```
GPUs | Theoretical Speedup | Actual Speedup | Efficiency
-----|---------------------|----------------|------------
1    | 1.0x                | 1.0x           | 100%
2    | 2.0x                | 1.85x          | 92.5%
4    | 4.0x                | 3.55x          | 88.8%
8    | 8.0x                | 6.80x          | 85.0%
```

**Key Files**: 2 files, ~230 lines
**Difficulty**: ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ (Distributed systems complexity)

---

#### Phase 5: Model Implementation (8-10 hours)
**What You'll Learn:**
- Transformer architecture optimizations
- Fused operations (SwiGLU, RMSNorm + residual)
- RoPE implementation details
- Weight loading and initialization strategies

**Concrete Deliverables:**
- Support for a new model (Llama3, Mistral, etc.)
- Custom fused kernel implementations
- Weight conversion scripts

**Key Files**: 5 files, ~520 lines
**Difficulty**: ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ (Standard ML engineering)

---

#### Phase 6: Advanced Optimizations (8-12 hours)
**What You'll Learn:**
- Prefix caching internals (xxHash, reference counting)
- CUDA graph capture and replay
- GPU memory planning and allocation
- When optimizations break (dynamic shapes, control flow)

**Concrete Deliverables:**
- Memory calculator tool (given model + GPU ‚Üí max sequences)
- Prefix caching effectiveness analysis
- CUDA graph coverage report

**Analysis Example:**
```
Workload: 1000 requests with 50% prompt overlap
Without prefix caching:  850ms avg latency
With prefix caching:     320ms avg latency (2.65x speedup)
Cache hit rate:          48.3%
Memory saved:            12.4 GB
```

**Key Files**: 2 files, ~366 lines
**Difficulty**: ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ (Deep GPU/CUDA knowledge needed)

---

#### Phase 7: Benchmarking & Profiling (6-8 hours)
**What You'll Learn:**
- Proper benchmarking methodology
- Profiling with torch.profiler and nvidia-smi
- Identifying bottlenecks in serving systems
- Parameter tuning for different workloads

**Concrete Deliverables:**
- Comprehensive benchmark suite
- Profiling automation scripts
- Performance tuning guide for different scenarios

**Workload Categories You'll Test:**
1. **Short prompts, long generation** (chatbot-style)
2. **Long prompts, short generation** (RAG/document QA)
3. **Variable length** (mixed workload)
4. **Repeated prompts** (prefix caching effectiveness)

**Key Files**: bench.py + custom scripts
**Difficulty**: ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ (Measurement science)

---

#### Phase 8: Real-World Connections (8-10 hours)
**What You'll Learn:**
- What full vLLM adds (quantization, continuous batching, PagedAttention evolution)
- Speculative decoding techniques
- Multi-LoRA serving
- Recent research directions (SGLang, FlashInfer)

**Concrete Deliverables:**
- Feature comparison matrix: nano-vLLM vs vLLM vs TGI vs TensorRT-LLM
- Implementation of one advanced feature (speculative decoding, quantization, etc.)
- Reading notes on 5+ key papers

**Key Papers to Study:**
1. vLLM/PagedAttention paper
2. Flash Attention (v1 and v2)
3. Speculative Decoding
4. SGLang (structured generation)
5. Continuous Batching (Orca paper)

**Difficulty**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Research-level understanding)

---

### Prerequisites

#### Required Knowledge
- ‚úÖ Strong Python (decorators, multiprocessing, async patterns)
- ‚úÖ PyTorch fundamentals (tensors, autograd, distributed training basics)
- ‚úÖ Transformer architecture (attention, MLP, layer norm)
- ‚úÖ GPU concepts (CUDA, memory hierarchy, parallelism)

#### Recommended Background
- Experience with production ML systems
- Understanding of batching and throughput optimization
- Familiarity with profiling tools
- Basic systems programming (memory management, process coordination)

#### Environment Requirements
- GPU with 16GB+ VRAM (RTX 4070 or better, A100/H100 ideal)
- PyTorch 2.x with CUDA support
- Flash Attention 2.x installed
- 32GB+ system RAM

---

### Comparison to Alternatives

#### Option A: Study Full vLLM Directly
**Pros**: Most complete implementation, production-ready
**Cons**: 50K+ lines, high complexity, hard to trace execution paths
**Time**: 4-6 weeks to reach similar understanding

#### Option B: Read Papers Only
**Pros**: Theoretical depth, broad coverage
**Cons**: No implementation experience, hard to internalize concepts
**Time**: 2-3 weeks, but less practical knowledge

#### Option C: This Nano-vLLM Path
**Pros**: Minimal complexity, real performance, hands-on learning
**Cons**: Limited model support, missing some advanced features
**Time**: 8-10 days to core mastery

**Efficiency Gain**: ~3x faster than full vLLM, ~2x more practical than papers-only

---

### Recommended Pacing

#### Intensive Mode (2 weeks full-time)
- Best for: Between jobs, sabbatical, focused learning period
- Schedule: 6-8 hours/day, 5 days/week
- Advantage: Maintain mental context across phases

#### Sustainable Mode (6-8 weeks part-time)
- Best for: While employed, steady skill building
- Schedule: 10-15 hours/week (evenings + weekends)
- Advantage: Time to digest concepts between phases

#### Hybrid Mode (4 weeks, 1 focused week + 3 part-time)
- Week 1: Phases 1-3 (intensive)
- Weeks 2-4: Phases 4-8 (part-time)
- Advantage: Quick foundation, then deep exploration

---

## Detailed Learning Path

### Phase 1: Architecture & System Design (Day 1)

Start with the big picture to understand how all pieces fit together:

#### 1. Read the README and run example.py
- Understand the value proposition: ~1200 lines achieving vLLM-level performance
- Run basic inference to get intuition

```bash
python example.py
```

#### 2. Study the request lifecycle (in order):
- `llm.py:31` - High-level `LLM.generate()` API
- `engine/llm_engine.py:21` - Main orchestrator
- `engine/sequence.py:10` - Request representation
- Trace a single request end-to-end

#### 3. Key Question to Answer
How does a prompt become tokens? Follow the flow:
```
generate() ‚Üí add_request() ‚Üí schedule() ‚Üí run() ‚Üí postprocess()
```

#### Exercises:
1. Add print statements to trace one request through the entire system
2. Draw a sequence diagram of the request flow
3. Identify which component owns each responsibility

---

### Phase 2: The Scheduling Problem (Day 2)

This is the heart of inference optimization:

#### 1. Deep dive: `engine/scheduler.py:10`
- Understand the prefill vs decode phase separation
- Study why prefill is compute-bound (high parallelism) vs decode is memory-bound (sequential)
- Analyze `max_num_batched_tokens` vs `max_num_seqs` constraints
- **Experiment**: Modify scheduling logic to prioritize different sequences

#### 2. Memory Management: `engine/block_manager.py:8`
- Study the 256-token block allocation strategy
- Understand prefix caching with xxHash (lines 28-55)
- See how reference counting enables zero-copy sharing
- **Key Insight**: Block-based caching is essential for memory efficiency

#### 3. Hands-on Task
Add logging to track:
- Cache hit rates
- Block allocation/deallocation patterns
- Preemption frequency

#### Exercises:
1. Calculate how many KV cache blocks are needed for:
   - 10 sequences of 1024 tokens each
   - 100 sequences of 512 tokens each
   - Mixed: 50 sequences (256 tokens) + 10 sequences (2048 tokens)

2. Modify the scheduler to implement different priorities:
   - FIFO (first-in-first-out)
   - Shortest-job-first
   - Longest-job-first
   - Compare throughput and latency

3. Implement cache hit rate monitoring:
```python
# Add to BlockManager
def get_cache_stats(self):
    return {
        'total_blocks': self.num_blocks,
        'allocated_blocks': len(self.allocated_blocks),
        'cache_hits': self.cache_hits,
        'cache_misses': self.cache_misses,
        'hit_rate': self.cache_hits / (self.cache_hits + self.cache_misses)
    }
```

---

### Phase 3: GPU Execution Optimization (Days 3-4)

The performance-critical path:

#### 1. Study: `engine/model_runner.py:31`
- Lines 112-140: Understand `prepare_prefill()` batching strategy
- Lines 142-165: See how `prepare_decode()` handles KV cache reuse
- Lines 202-253: **CUDA Graphs** - this is where magic happens

#### 2. Flash Attention Integration: `layers/attention.py:11`
- See `flash_attn_varlen_func` for prefill (variable-length sequences)
- See `flash_attn_with_kvcache` for decode (cached attention)
- Understand why Flash Attention is 3-5x faster

#### 3. Torch Compile Usage
Study these optimized kernels:
- `layers/sampler.py:10` - Temperature sampling with Gumbel trick
- `layers/rotary_embedding.py:11` - RoPE implementation
- `layers/layernorm.py:9` - Fused RMSNorm
- **Pattern**: Small, frequently-called kernels benefit most from `@torch.compile`

#### 4. Experiments

**Experiment 1: CUDA Graphs Impact**
```bash
# Without CUDA graphs
python bench.py --model <model> --enforce-eager --num-prompts 128

# With CUDA graphs
python bench.py --model <model> --num-prompts 128
```

**Experiment 2: Profiling**
```python
import torch
from torch.profiler import profile, ProfilerActivity

with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:
    outputs = llm.generate(prompts, sampling_params)

print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
```

**Experiment 3: Flash Attention Impact**
- Temporarily replace Flash Attention with naive attention
- Measure the performance difference
- Analyze memory usage

#### Exercises:
1. Measure kernel-level performance:
   - Attention computation time
   - MLP forward pass time
   - Sampling time
   - Identify the bottleneck

2. Vary batch sizes and measure:
   - GPU utilization (use `nvidia-smi dmon`)
   - Throughput (tokens/sec)
   - Latency (ms per token)

3. Create a performance report:
```
Configuration          | Tokens/sec | GPU Util | Latency p50 | Latency p99
-----------------------|------------|----------|-------------|-------------
Batch=1, Eager         |            |          |             |
Batch=8, Eager         |            |          |             |
Batch=32, Eager        |            |          |             |
Batch=1, CUDA Graph    |            |          |             |
Batch=8, CUDA Graph    |            |          |             |
Batch=32, CUDA Graph   |            |          |             |
```

---

### Phase 4: Tensor Parallelism (Day 5)

Multi-GPU scaling techniques:

#### 1. Study parallelism patterns in `layers/linear.py:10`
- `ColumnParallelLinear:21` - Output feature splitting
- `RowParallelLinear:54` - Input feature splitting with all-reduce
- `QKVParallelLinear:88` - Query/Key/Value projection sharding

#### 2. Process coordination: `engine/model_runner.py:31`
- Lines 31-77: Worker process spawning with multiprocessing
- SharedMemory communication pattern
- NCCL synchronization with `dist.all_reduce()`

#### 3. Hands-on
Run with `tensor_parallel_size=2` or more and trace:
- How weights are sharded (use `utils/loader.py:39`)
- How intermediate activations are communicated
- Measure scaling efficiency

#### Exercises:
1. Implement a new parallel layer:
```python
class CustomParallelLinear(nn.Module):
    """Your implementation of a tensor parallel linear layer"""
    def __init__(self, in_features, out_features, tp_size, tp_rank):
        # Implement weight sharding
        pass

    def forward(self, x):
        # Implement forward pass with communication
        pass
```

2. Measure scaling efficiency:
```bash
# 1 GPU
python bench.py --model <model> --tensor-parallel-size 1

# 2 GPUs
python bench.py --model <model> --tensor-parallel-size 2

# 4 GPUs
python bench.py --model <model> --tensor-parallel-size 4

# Calculate: actual_speedup / theoretical_speedup
```

3. Profile communication overhead:
   - Time spent in all_reduce operations
   - Data transfer sizes
   - Communication vs computation ratio

---

### Phase 5: Model Implementation Details (Day 6)

Understanding the Transformer stack:

#### 1. Study: `models/qwen3.py:9`
- Qwen3Attention (lines 17-61): RoPE + Flash Attention
- Qwen3MLP (lines 65-80): SwiGLU activation
- Qwen3DecoderLayer (lines 84-108): Pre-norm residual structure
- Qwen3ForCausalLM (lines 143-183): Weight tying

#### 2. Key optimizations to note
- `layers/activation.py:9` - Fused SiLU and multiply
- `layers/layernorm.py:31` - Fused residual + RMSNorm
- Weight loader callbacks for flexible initialization

#### Exercises:
1. Add support for a new model (e.g., Llama3):
   - Study the architecture differences
   - Implement the attention, MLP, and decoder layers
   - Test with a small model

2. Implement a fused operation:
```python
@torch.compile
def fused_bias_gelu(x, bias):
    """Fuse bias addition and GELU activation"""
    # Your implementation
    pass
```

3. Benchmark your implementations:
   - Compare fused vs unfused operations
   - Measure memory savings
   - Profile execution time

---

### Phase 6: Advanced Optimization Techniques (Day 7)

Deep dives into specific optimizations:

#### 1. Prefix Caching Implementation
- Study hash computation in `engine/block_manager.py:28-42`
- Understand when cache hits occur
- Calculate memory savings for repeated prompts

#### 2. CUDA Graph Capture
- Study `engine/model_runner.py:202-253`
- Understand static vs dynamic graphs
- Learn when graphs can't be used (dynamic shapes)

#### 3. Memory Planning
- Study `config.py:11` - `gpu_memory_utilization` parameter
- See KV cache allocation in `engine/model_runner.py:78-102`
- Calculate theoretical max sequences for given model/GPU

#### Exercises:
1. Create a memory calculator:
```python
def calculate_kv_cache_memory(
    num_layers: int,
    num_heads: int,
    head_dim: int,
    max_seq_len: int,
    max_batch_size: int,
    dtype: str = "float16"
) -> int:
    """Calculate KV cache memory in bytes"""
    # Your implementation
    pass

def max_sequences_for_gpu(
    model_params: dict,
    gpu_memory_gb: int,
    gpu_memory_utilization: float = 0.9
) -> int:
    """Calculate maximum concurrent sequences"""
    # Your implementation
    pass
```

2. Analyze prefix caching effectiveness:
```python
# Create test workload with varying prompt overlap
prompts = [
    "What is the capital of France?",
    "What is the capital of Germany?",
    "What is the capital of Italy?",
    # Many prompts sharing "What is the capital of"
]

# Measure cache hits, memory savings, latency improvement
```

3. CUDA graph coverage analysis:
   - Which batch sizes have captured graphs?
   - What percentage of execution uses graphs?
   - What's the CPU overhead reduction?

---

### Phase 7: Benchmarking & Profiling (Day 8)

Measure everything:

#### 1. Run comprehensive benchmarks
```bash
# Different workload patterns
python bench.py --model <model> --num-prompts 256 --input-len 128 --output-len 128
python bench.py --model <model> --num-prompts 256 --input-len 512 --output-len 64
python bench.py --model <model> --num-prompts 256 --input-len 64 --output-len 512
```

#### 2. Profile bottlenecks
```python
# Add detailed profiling
import torch.profiler

with torch.profiler.profile(
    activities=[
        torch.profiler.ProfilerActivity.CPU,
        torch.profiler.ProfilerActivity.CUDA,
    ],
    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3),
    on_trace_ready=torch.profiler.tensorboard_trace_handler('./log'),
    record_shapes=True,
    profile_memory=True,
    with_stack=True
) as prof:
    # Run inference
    outputs = llm.generate(prompts, sampling_params)
    prof.step()

# Analyze with TensorBoard
# tensorboard --logdir=./log
```

#### 3. Parameter tuning experiments
Test different configurations:
- `max_num_batched_tokens`: [4096, 8192, 16384, 32768]
- `max_num_seqs`: [128, 256, 512, 1024]
- `kvcache_block_size`: [128, 256, 512]
- `gpu_memory_utilization`: [0.7, 0.8, 0.9, 0.95]

#### Exercises:
1. Create a comprehensive benchmark suite:
```python
# bench_suite.py
workloads = [
    {"name": "chatbot", "input_len": 128, "output_len": 256},
    {"name": "rag_qa", "input_len": 2048, "output_len": 64},
    {"name": "code_gen", "input_len": 512, "output_len": 512},
    {"name": "summarization", "input_len": 4096, "output_len": 256},
]

for workload in workloads:
    # Run benchmark and collect metrics
    pass
```

2. Identify bottlenecks:
   - What operation takes the most time?
   - Where is memory bandwidth saturated?
   - What's the GPU utilization?
   - Is the system CPU-bound or GPU-bound?

3. Create a tuning guide:
   - For throughput-optimized serving
   - For latency-optimized serving
   - For memory-constrained environments
   - For high-concurrency scenarios

---

### Phase 8: Real-World Connections (Days 9-10)

Bridge to production systems:

#### 1. Compare with full vLLM
- Understand what nano-vLLM simplifies (no quantization, limited model support)
- Study additional vLLM features:
  - Continuous batching
  - Speculative decoding
  - PagedAttention
  - Multi-LoRA serving
  - Quantization (AWQ, GPTQ, FP8)

#### 2. Read relevant papers
1. **vLLM/PagedAttention**: "Efficient Memory Management for Large Language Model Serving with PagedAttention"
2. **Flash Attention v1**: "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"
3. **Flash Attention v2**: "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"
4. **Speculative Decoding**: "Fast Inference from Transformers via Speculative Decoding"
5. **Orca (Continuous Batching)**: "Orca: A Distributed Serving System for Transformer-Based Generative Models"
6. **SGLang**: "SGLang: Efficient Execution of Structured Language Model Programs"

#### 3. Extend nano-vLLM (choose one project)

**Project Option 1: Add support for another model**
- Implement Llama3, Mistral, or Gemma support
- Handle architecture differences
- Test with real weights

**Project Option 2: Implement speculative decoding**
- Add draft model support
- Implement verification logic
- Measure speedup on different tasks

**Project Option 3: Add quantization support**
- Implement INT8 or FP8 quantization
- Modify linear layers for quantized weights
- Measure accuracy vs speed tradeoff

**Project Option 4: Implement continuous batching**
- Allow dynamic request arrival
- Implement more flexible scheduling
- Measure latency improvements

#### Exercises:
1. Create a feature comparison matrix:
```
Feature                    | nano-vLLM | vLLM | TGI | TensorRT-LLM
---------------------------|-----------|------|-----|---------------
Prefix Caching             |     ‚úì     |   ‚úì  |  ‚úì  |      ‚úì
CUDA Graphs                |     ‚úì     |   ‚úì  |  ‚úó  |      ‚úì
Flash Attention            |     ‚úì     |   ‚úì  |  ‚úì  |      ‚úì
Tensor Parallelism         |     ‚úì     |   ‚úì  |  ‚úì  |      ‚úì
Pipeline Parallelism       |     ‚úó     |   ‚úì  |  ‚úì  |      ‚úì
Quantization               |     ‚úó     |   ‚úì  |  ‚úì  |      ‚úì
Speculative Decoding       |     ‚úó     |   ‚úì  |  ‚úó  |      ‚úì
Multi-LoRA                 |     ‚úó     |   ‚úì  |  ‚úì  |      ‚úó
Continuous Batching        |     ‚úó     |   ‚úì  |  ‚úì  |      ‚úì
```

2. Implement one advanced feature end-to-end

3. Write a technical blog post explaining:
   - What you learned
   - Key insights from implementation
   - Performance characteristics
   - Comparison with production systems

---

## Success Metrics

### After Phase 4 (Minimum Viable Understanding)

You can:
- ‚úÖ Explain why LLM serving is memory-bound
- ‚úÖ Calculate KV cache requirements
- ‚úÖ Implement basic tensor parallelism
- ‚úÖ Profile and identify bottlenecks

**Job-Ready For**: ML Platform Engineer, ML Infrastructure roles

### After Phase 8 (Full Mastery)

You can:
- ‚úÖ Design production serving systems from scratch
- ‚úÖ Evaluate tradeoffs between serving frameworks
- ‚úÖ Contribute to vLLM/SGLang/similar projects
- ‚úÖ Lead inference optimization initiatives

**Job-Ready For**: Staff+ ML Infrastructure, Principal SDE (Inference), Founding Engineer at AI startups

---

## Critical Concepts to Master

### 1. Prefill vs Decode Phase Separation
**Why it matters**: These phases have completely different characteristics
- **Prefill**: Compute-bound, high parallelism, processes all prompt tokens
- **Decode**: Memory-bound, sequential, generates one token at a time
- **Implication**: Need different optimization strategies for each phase

### 2. KV Cache Management
**Why it matters**: Memory is the primary bottleneck in LLM serving
- Block-based allocation prevents fragmentation
- Prefix caching enables zero-copy sharing
- Reference counting tracks shared blocks
- **Implication**: Efficient memory management = higher throughput

### 3. CUDA Graphs
**Why it matters**: Eliminates CPU overhead in the critical path
- Pre-captures static computation graphs
- Replays graphs without CPU involvement
- ~10-20% speedup in decode phase
- **Implication**: Essential for low-latency serving

### 4. Flash Attention
**Why it matters**: Memory-efficient attention is essential
- 3-5x faster than naive attention
- Reduces memory bandwidth requirements
- Enables longer sequences
- **Implication**: Must-have for production systems

### 5. Batching Strategies
**Why it matters**: Throughput comes from efficient batching
- Dynamic batching increases GPU utilization
- Variable-length batching avoids padding waste
- Scheduling algorithms balance throughput vs latency
- **Implication**: Good scheduling = 5-10x throughput improvement

### 6. Tensor Parallelism
**Why it matters**: Scale to multi-GPU without changing code logic
- Shard model weights across GPUs
- Minimize communication overhead
- Enable serving of large models
- **Implication**: Path to scaling beyond single GPU

---

## Risk Assessment & Mitigation

### Potential Blockers

#### 1. GPU Access
**Risk**: Need 16GB+ GPU for realistic experiments
**Mitigation**:
- Use cloud instances (A100 on Lambda Labs ~$1/hr)
- Start with smaller models on consumer GPUs
- Phase 1-3 can be done with 8GB GPU

#### 2. Flash Attention Installation
**Risk**: Can be tricky with PyTorch versions
**Mitigation**:
- You've already solved this!
- Document the working setup
- Use pre-built wheels when available

#### 3. CUDA Knowledge Gaps
**Risk**: Some optimizations require GPU programming background
**Mitigation**:
- Focus on usage patterns first
- Deep-dive CUDA in Phase 6
- Treat CUDA graphs as black boxes initially

#### 4. Time Commitment
**Risk**: 60-80 hours is significant
**Mitigation**:
- Phases 1-4 give 80% of value in 40 hours
- Can pause after Phase 4 and resume later
- Use sustainable pacing mode

---

## Additional Resources

### Documentation & Tutorials
- [vLLM Documentation](https://docs.vllm.ai/)
- [Flash Attention GitHub](https://github.com/Dao-AILab/flash-attention)
- [PyTorch Distributed](https://pytorch.org/tutorials/beginner/dist_overview.html)
- [CUDA Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)

### Papers (in reading order)
1. Flash Attention v1 (arXiv:2205.14135)
2. Flash Attention v2 (arXiv:2307.08691)
3. vLLM/PagedAttention (arXiv:2309.06180)
4. Orca/Continuous Batching (OSDI 2022)
5. Speculative Decoding (arXiv:2211.17192)
6. SGLang (arXiv:2312.07104)

### Community Resources
- [vLLM GitHub Discussions](https://github.com/vllm-project/vllm/discussions)
- [CUDA MODE Discord](https://discord.gg/cudamode)
- [PyTorch Forums](https://discuss.pytorch.org/)

### Tools for Profiling
- `torch.profiler` - PyTorch's built-in profiler
- `nvidia-smi dmon` - GPU monitoring
- `nsys` - Nvidia Nsight Systems
- `ncu` - Nvidia Nsight Compute
- TensorBoard - Visualization

---

## Next Steps

### Getting Started Today

1. **Environment Verification** (30 mins)
```bash
# Check GPU
nvidia-smi

# Check PyTorch + CUDA
python -c "import torch; print(torch.cuda.is_available())"

# Check Flash Attention
python -c "from flash_attn import flash_attn_func; print('OK')"

# Run example
python example.py
```

2. **Phase 1 Start** (Today)
- Read `llm.py` and understand the public API
- Study `engine/llm_engine.py` to see the orchestration
- Run example.py with added print statements

3. **Set Up Learning Journal**
```bash
mkdir learning_journal
echo "# Nano-vLLM Learning Journal" > learning_journal/README.md
echo "## Phase 1: $(date)" >> learning_journal/README.md
```

### Recommended Schedule Options

#### Option 1: Intensive (2 weeks)
```
Week 1:
  Mon: Phase 1
  Tue: Phase 2
  Wed: Phase 3 (part 1)
  Thu: Phase 3 (part 2)
  Fri: Phase 4

Week 2:
  Mon: Phase 5
  Tue: Phase 6
  Wed: Phase 7
  Thu-Fri: Phase 8
```

#### Option 2: Sustainable (6-8 weeks)
```
Week 1-2: Phase 1-2
Week 3-4: Phase 3-4
Week 5-6: Phase 5-6
Week 7-8: Phase 7-8
```

#### Option 3: Hybrid (4 weeks)
```
Week 1 (intensive): Phases 1-3
Week 2-4 (part-time): Phases 4-8
```

---

## Conclusion

This learning path provides a systematic approach to mastering LLM inference optimization through the nano-vLLM codebase. By the end, you'll have:

- **Deep technical understanding** of modern serving systems
- **Hands-on experience** implementing optimizations
- **Practical skills** for production inference engineering
- **Foundation** for contributing to major projects like vLLM

The key to success is:
1. **Hands-on experimentation** - Don't just read, implement and measure
2. **Systematic progression** - Complete each phase before moving forward
3. **Document learnings** - Keep notes for future reference
4. **Connect concepts** - Understand why each optimization matters

Good luck on your learning journey! üöÄ

---

## Appendix: Quick Reference

### Key File Reference
```
llm.py                      # High-level API
engine/llm_engine.py        # Main orchestrator (94 lines)
engine/scheduler.py         # Request scheduling (72 lines)
engine/model_runner.py      # Model execution (253 lines)
engine/block_manager.py     # KV cache management (113 lines)
engine/sequence.py          # Request representation
layers/attention.py         # FlashAttention (76 lines)
layers/linear.py            # Tensor parallelism (154 lines)
models/qwen3.py             # Model implementation (217 lines)
config.py                   # Configuration
sampling_params.py          # Sampling parameters
```

### Configuration Parameters
```python
max_num_batched_tokens: int = 16384    # Max tokens per batch
max_num_seqs: int = 512                # Max parallel sequences
max_model_len: int = 4096              # Max sequence length
gpu_memory_utilization: float = 0.9    # GPU memory target
tensor_parallel_size: int = 1          # TP degree
enforce_eager: bool = False            # Disable CUDA graphs
kvcache_block_size: int = 256          # KV cache block size
```

### Common Commands
```bash
# Run basic example
python example.py

# Run benchmark
python bench.py --model <model> --num-prompts 256

# With profiling
python -m torch.utils.bottleneck bench.py

# Monitor GPU
nvidia-smi dmon -s u

# Profile with nsys
nsys profile -o profile python bench.py
```

### Debugging Tips
```python
# Add logging
import logging
logging.basicConfig(level=logging.DEBUG)

# Trace execution
import sys
sys.settrace(lambda *args: print(args))

# Profile specific section
import time
start = time.perf_counter()
# ... code ...
print(f"Elapsed: {time.perf_counter() - start:.3f}s")
```
